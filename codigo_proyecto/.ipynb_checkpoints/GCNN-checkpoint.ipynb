{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[RUNME] Install Colab TPU compatible PyTorch/TPU wheels and dependencie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import requests\n",
    "import threading\n",
    "\n",
    "_VersionConfig = collections.namedtuple('_VersionConfig', 'wheels,server')\n",
    "VERSION = \"xrt==1.15.0\"  #@param [\"xrt==1.15.0\", \"torch_xla==nightly\"]\n",
    "CONFIG = {\n",
    "    'xrt==1.15.0': _VersionConfig('1.15', '1.15.0'),\n",
    "    'torch_xla==nightly': _VersionConfig('nightly', 'XRT-dev{}'.format(\n",
    "        (datetime.today() - timedelta(1)).strftime('%Y%m%d'))),\n",
    "}[VERSION]\n",
    "DIST_BUCKET = 'gs://tpu-pytorch/wheels'\n",
    "TORCH_WHEEL = 'torch-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
    "TORCH_XLA_WHEEL = 'torch_xla-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
    "TORCHVISION_WHEEL = 'torchvision-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
    "\n",
    "# Update TPU XRT version\n",
    "def update_server_xrt():\n",
    "  print('Updating server-side XRT to {} ...'.format(CONFIG.server))\n",
    "  url = 'http://{TPU_ADDRESS}:8475/requestversion/{XRT_VERSION}'.format(\n",
    "      TPU_ADDRESS=os.environ['COLAB_TPU_ADDR'].split(':')[0],\n",
    "      XRT_VERSION=CONFIG.server,\n",
    "  )\n",
    "  print('Done updating server-side XRT: {}'.format(requests.post(url)))\n",
    "\n",
    "update = threading.Thread(target=update_server_xrt)\n",
    "update.start()\n",
    "\n",
    "# Install Colab TPU compat PyTorch/TPU wheels and dependencies\n",
    "!pip uninstall -y torch torchvision\n",
    "!gsutil cp \"$DIST_BUCKET/$TORCH_WHEEL\" .\n",
    "!gsutil cp \"$DIST_BUCKET/$TORCH_XLA_WHEEL\" .\n",
    "!gsutil cp \"$DIST_BUCKET/$TORCHVISION_WHEEL\" .\n",
    "!pip install \"$TORCH_WHEEL\"\n",
    "!pip install \"$TORCH_XLA_WHEEL\"\n",
    "!pip install \"$TORCHVISION_WHEEL\"\n",
    "!sudo apt-get install libomp5\n",
    "update.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importar proyecto desde github\n",
    "!git clone https://github.com/Camiloez/nlp_project.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setear directorio de trabajo\n",
    "import os\n",
    "os.chdir('nlp_project/codigo_proyecto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torch.autograd import Variable\n",
    "from fastai.text import TextLMDataBunch, collections\n",
    "\n",
    "from gcnn import GCNNmodel\n",
    "from config import params\n",
    "from GCNN_textfuncs import LMDataset_GCNN, SortishSampler_GCNN, SortSampler_GCNN, pad_collate_GCNN\n",
    "from tools import make_paragraphs, make_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device =  xm.xla_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epochs': 50,\n",
       " 'emb_sz': 300,\n",
       " 'k': 4,\n",
       " 'nh': 600,\n",
       " 'nl': 4,\n",
       " 'downbot': 20,\n",
       " 'batch_size': 20,\n",
       " 'lr': 1,\n",
       " 'mom': 0.95,\n",
       " 'wd': 5e-05,\n",
       " 'nesterov': True,\n",
       " 'grad_clip': 0.07,\n",
       " 'opttype': 'sgd',\n",
       " 'use_gpu': True}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. WikiText 103 dataset processing <a class=\"anchor\" id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos los archivos a partir de un .zip, el .zip cuenta con tres archivos planos, train, valid y test. De cada archivo se extraen los documentos y son guardados en una lista dentro de un diccionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#montar drive donde se halla el dataset y los embeddings\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importar datasets y guardar en un diccionario\n",
    "shutil.unpack_archive('../wikitext-103-v1.zip', extract_dir='dataset')\n",
    "working_dir = os.path.join(os.getcwd(), 'dataset', 'wikitext-103')\n",
    "wikitext_files = os.listdir(working_dir)\n",
    "\n",
    "wiki = {}\n",
    "for wikitext_file in wikitext_files:\n",
    "    with open(os.path.join(working_dir, wikitext_file), encoding='utf-8') as data_file:\n",
    "        name = wikitext_file.split('.')[1]\n",
    "        corpus = []\n",
    "        for index, line in enumerate(data_file):\n",
    "            # filtrar lineas vacías y headers\n",
    "            if len(line) < 3 or line[1] == '=':\n",
    "                continue\n",
    "            else:\n",
    "                corpus.append(line.strip()+' </s>')#añadir end symbol\n",
    "        #list of str: dataset no tokenizado        \n",
    "        wiki[name] = corpus\n",
    "shutil.rmtree('dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exportar data como .csv\n",
    "df_train = pd.DataFrame({'text':wiki['train']})\n",
    "df_valid = pd.DataFrame({'text':wiki['valid']})\n",
    "df_test = pd.DataFrame({'text':wiki['test']})\n",
    "\n",
    "#Etiquetas se agregan para cargar datos despues\n",
    "df_train['labels'] = 0\n",
    "df_valid['labels'] = 0\n",
    "df_test['labels'] = 0\n",
    "\n",
    "df_train = df_train[['labels','text']]\n",
    "df_valid = df_valid[['labels','text']]\n",
    "df_test = df_test[['labels','text']]\n",
    "\n",
    "df_train.to_csv('train.csv', header=False, index=False, sep='|')\n",
    "df_valid.to_csv('valid.csv', header=False, index=False, sep='|')\n",
    "df_test.to_csv('test.csv', header=False, index=False, sep='|')\n",
    "\n",
    "#Liberar RAM\n",
    "del wiki, df_train, df_valid, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar datos con fastai.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm = TextLMDataBunch.from_csv(path = '', delimiter = '|', csv_name='train.csv', test='test.csv', \n",
    "          max_vocab=300000, min_freq=0)\n",
    "itos=data_lm.train_ds.vocab.itos\n",
    "vocab_size=len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens=[data_lm.train_ds[i][0].data for i in range(len(data_lm.train_ds))]\n",
    "test_tokens=[data_lm.valid_ds[i][0].data for i in range(len(data_lm.valid_ds))]\n",
    "\n",
    "train_dataset = LMDataset_GCNN(train_tokens)\n",
    "test_dataset = LMDataset_GCNN(test_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear DataLoader y Samplers\n",
    "\n",
    "Los siguientes pasos son muy importantes para poder entrenar el modelo.\n",
    "\n",
    "- El Sampler se encarga de generar las muestras de manera correcta en el DataLoader\n",
    "- El DataLoader se encarga de cargar bien los datos para poder manejarlos en memoria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Samplers\n",
    "train_sampler = SortishSampler_GCNN(data_length = len(train_dataset),key=lambda x:len(train_dataset[x][0]), bs = params['batch_size'])\n",
    "test_sampler = SortSampler_GCNN(test_dataset,key = lambda x:len(test_dataset[x][0]))\n",
    "\n",
    "#DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size = params['batch_size'], collate_fn=pad_collate_GCNN, sampler = train_sampler, pin_memory= False)\n",
    "test_loader = DataLoader(test_dataset, batch_size= params['batch_size'], collate_fn=pad_collate_GCNN, sampler = test_sampler, pin_memory= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embeddings(path, itos, embedding_size):\n",
    "    \"\"\" A partir del path de un .txt con embeddings genera un vector numpy de embeddings\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    word2idx = {}\n",
    "    vectors = []\n",
    "    idx = 0\n",
    "    with open(path, 'rb') as file:\n",
    "        for line in file:\n",
    "            line = line.decode().split()\n",
    "            word = line[0]\n",
    "            words.append(word)\n",
    "            word2idx[word] = idx\n",
    "            idx += 1\n",
    "            vectors.append(line[1:])\n",
    "    \n",
    "    #Verificamos que tengamos misma cantidad de palabras y vectores\n",
    "    assert len(words) == len(vectors)\n",
    "    \n",
    "    #Generamos diccionario\n",
    "    temporal_dictionary = collections.defaultdict(lambda:-1, {v:k for k,v in enumerate(words)})\n",
    "    unk_row = vectors[-1] # vector default para <unk>\n",
    "    #Creamos vector de tamaño número de palabras, por tamaño de embedding\n",
    "    embeddings = np.zeros((len(itos), embedding_size), dtype=np.float32)\n",
    "    \n",
    "    #Vamos iterando por palabras del diccionario que creamos antes para generar vector de embeddings con la misma\n",
    "    #correspondencia de indices\n",
    "    for i, word in enumerate(itos):\n",
    "        index = temporal_dictionary[word] #indice de palabra en diccionario temporal de embeddings cargados\n",
    "        #Generamos el vector de embeddings \n",
    "        embeddings[i] = vectors[index] if index>=0 else unk_row #Si no lo encuentra asigna vector por defecto \n",
    "        \n",
    "    return embeddings\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'glove.6B.300d.txt'\n",
    "embeddings = make_embeddings(path, itos, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Primero generamos el modelo, los párametros están en config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCNN = GCNNmodel(vocab_size, params['emb_sz'], params['k'], params['nh'], params['nl'], params['downbot'])\n",
    "GCNN.model.embed.weight.data = torch.FloatTensor(embeddings).to(xm.xla_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCNNmodel(\n",
       "  (embed): Embedding(280072, 300)\n",
       "  (inlayer): GLUblock(\n",
       "    (convresid): Conv2d(300, 600, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (leftpad): ConstantPad2d(padding=(0, 0, 3, 0), value=0)\n",
       "    (convx1a): Conv2d(300, 15, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (convx2a): Conv2d(300, 15, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (convx1b): Conv2d(15, 15, kernel_size=(4, 1), stride=(1, 1))\n",
       "    (convx2b): Conv2d(15, 15, kernel_size=(4, 1), stride=(1, 1))\n",
       "    (convx1c): Conv2d(15, 600, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (convx2c): Conv2d(15, 600, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (GLUlayers): Sequential(\n",
       "    (0): GLUblock(\n",
       "      (convresid): Conv2d(600, 600, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (leftpad): ConstantPad2d(padding=(0, 0, 3, 0), value=0)\n",
       "      (convx1a): Conv2d(600, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (convx2a): Conv2d(600, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (convx1b): Conv2d(30, 30, kernel_size=(4, 1), stride=(1, 1))\n",
       "      (convx2b): Conv2d(30, 30, kernel_size=(4, 1), stride=(1, 1))\n",
       "      (convx1c): Conv2d(30, 600, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (convx2c): Conv2d(30, 600, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (1): GLUblock(\n",
       "      (convresid): Conv2d(600, 600, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (leftpad): ConstantPad2d(padding=(0, 0, 3, 0), value=0)\n",
       "      (convx1a): Conv2d(600, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (convx2a): Conv2d(600, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (convx1b): Conv2d(30, 30, kernel_size=(4, 1), stride=(1, 1))\n",
       "      (convx2b): Conv2d(30, 30, kernel_size=(4, 1), stride=(1, 1))\n",
       "      (convx1c): Conv2d(30, 600, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (convx2c): Conv2d(30, 600, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (2): GLUblock(\n",
       "      (convresid): Conv2d(600, 600, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (leftpad): ConstantPad2d(padding=(0, 0, 3, 0), value=0)\n",
       "      (convx1a): Conv2d(600, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (convx2a): Conv2d(600, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (convx1b): Conv2d(30, 30, kernel_size=(4, 1), stride=(1, 1))\n",
       "      (convx2b): Conv2d(30, 30, kernel_size=(4, 1), stride=(1, 1))\n",
       "      (convx1c): Conv2d(30, 600, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (convx2c): Conv2d(30, 600, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (3): GLUblock(\n",
       "      (convresid): Conv2d(600, 600, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (leftpad): ConstantPad2d(padding=(0, 0, 3, 0), value=0)\n",
       "      (convx1a): Conv2d(600, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (convx2a): Conv2d(600, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (convx1b): Conv2d(30, 30, kernel_size=(4, 1), stride=(1, 1))\n",
       "      (convx2b): Conv2d(30, 30, kernel_size=(4, 1), stride=(1, 1))\n",
       "      (convx1c): Conv2d(30, 600, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (convx2c): Conv2d(30, 600, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (out): AdaptiveLogSoftmaxWithLoss(\n",
       "    (head): Linear(in_features=600, out_features=11205, bias=False)\n",
       "    (tail): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=600, out_features=150, bias=False)\n",
       "        (1): Linear(in_features=150, out_features=44811, bias=False)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=600, out_features=37, bias=False)\n",
       "        (1): Linear(in_features=37, out_features=224058, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Escribir output en un .txt\n",
    "with open('/content/drive/My Drive/data/log.txt', 'w') as f:\n",
    "    start_time = time.time()\n",
    "    train_loss_list = []; test_loss_list = []\n",
    "    for epoch in range(params['epochs']):\n",
    "        ti = time.time()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "            #Optimizer\n",
    "            if params['opttype'] == 'sgd':\n",
    "                optimizer = torch.optim.SGD(GCNN.parameters(), lr = params['lr'], momentum=params['mom'], weight_decay=params['wd'],\n",
    "                                    nesterov= params['nesterov'])\n",
    "            elif params['opttype'] == 'adam':\n",
    "                optimizer = torch.optim.Adam(GCNN.parameters(), lr = params['lr'], betas = (params['mom'], 0.999))\n",
    "\n",
    "            #Dejar los gradientes del optimizador en 0\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #Forward pass\n",
    "            output = GCNN(data.to(device))\n",
    "\n",
    "            #Actualizar párametros\n",
    "            loss = output.loss\n",
    "            loss.backward()\n",
    "\n",
    "            train_loss_list.append(loss.item())\n",
    "\n",
    "            #Gradient clipping\n",
    "            if params['grad_clip'] != 0:\n",
    "                nn.utils.clip_grad_value_(GCNN.parameters(), params['grad_clip'])\n",
    "\n",
    "            #Actualizar pesos\n",
    "            xm.optimizer_step(optimizer, barrier=True)\n",
    "\n",
    "            if batch_idx % 10000 == 0:\n",
    "                elapsed_time=time.time()-start_time\n",
    "                batch_info = 'Epoch: {}  Batches: {}  Loss: {} Time: {}s'.format(epoch, batch_idx, loss.item(), elapsed_time)\n",
    "                print(batch_info)\n",
    "                f.write(batch_info+'\\n')\n",
    "\n",
    "        #Verificamos ahora en el test set\n",
    "        val_loss=[]\n",
    "\n",
    "        #Gradientes no se actualizan y modelo en modo evaluacion\n",
    "        with torch.no_grad():\n",
    "            GCNN.eval()\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(test_loader):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                #Forward\n",
    "                output = GCNN(data.to(device))\n",
    "                loss = output.loss\n",
    "                output.output\n",
    "                val_loss.append(loss.data.item())\n",
    "\n",
    "        #Modelo en modo entrenamiento\n",
    "        GCNN.train().to(device)\n",
    "        ave_val_loss=sum(val_loss) / len(val_loss)\n",
    "        val_update_string='Validation Loss: {:.4f}\\tPerp: {:.4f}'.format(ave_val_loss,np.exp(ave_val_loss))\n",
    "        print(val_update_string+'\\n')\n",
    "        f.write(val_update_string)\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = f'Total time: {end_time-start_time}s'\n",
    "    f.write(total_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
