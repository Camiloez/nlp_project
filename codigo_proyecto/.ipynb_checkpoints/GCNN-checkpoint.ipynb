{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torch.autograd import Variable\n",
    "from fastai.text import TextLMDataBunch, collections\n",
    "\n",
    "from gcnn import GCNNmodel\n",
    "from config import params\n",
    "from GCNN_textfuncs import LMDataset_GCNN, SortishSampler_GCNN, SortSampler_GCNN, pad_collate_GCNN\n",
    "from tools import make_paragraphs, make_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epochs': 50,\n",
       " 'emb_sz': 300,\n",
       " 'k': 4,\n",
       " 'nh': 600,\n",
       " 'nl': 4,\n",
       " 'downbot': 20,\n",
       " 'batch_size': 20,\n",
       " 'lr': 1,\n",
       " 'mom': 0.95,\n",
       " 'wd': 5e-05,\n",
       " 'nesterov': True,\n",
       " 'grad_clip': 0.07,\n",
       " 'opttype': 'sgd',\n",
       " 'use_gpu': True}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. WikiText 103 dataset processing <a class=\"anchor\" id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos los archivos a partir de un .zip, el .zip cuenta con tres archivos planos, train, valid y test. De cada archivo se extraen los documentos y son guardados en una lista dentro de un diccionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importar datasets y guardar en un diccionario\n",
    "shutil.unpack_archive('../wikitext-103-v1.zip', extract_dir='dataset')\n",
    "working_dir = os.path.join(os.getcwd(), 'dataset', 'wikitext-103')\n",
    "wikitext_files = os.listdir(working_dir)\n",
    "\n",
    "wiki = {}\n",
    "for wikitext_file in wikitext_files:\n",
    "    with open(os.path.join(working_dir, wikitext_file), encoding='utf-8') as data_file:\n",
    "        name = wikitext_file.split('.')[1]\n",
    "        corpus = []\n",
    "        for index, line in enumerate(data_file):\n",
    "            # filtrar lineas vacías y headers\n",
    "            if len(line) < 3 or line[1] == '=':\n",
    "                continue\n",
    "            else:\n",
    "                corpus.append(line.strip()+' </s>')#añadir end symbol\n",
    "        #list of str: dataset no tokenizado        \n",
    "        wiki[name] = corpus\n",
    "shutil.rmtree('dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exportar data como .csv\n",
    "df_train = pd.DataFrame({'text':wiki['train']})\n",
    "df_valid = pd.DataFrame({'text':wiki['valid']})\n",
    "df_test = pd.DataFrame({'text':wiki['test']})\n",
    "\n",
    "#Etiquetas se agregan para cargar datos despues\n",
    "df_train['labels'] = 0\n",
    "df_valid['labels'] = 0\n",
    "df_test['labels'] = 0\n",
    "\n",
    "df_train = df_train[['labels','text']]\n",
    "df_valid = df_valid[['labels','text']]\n",
    "df_test = df_test[['labels','text']]\n",
    "\n",
    "df_train.to_csv('train.csv', header=False, index=False, sep='|')\n",
    "df_valid.to_csv('valid.csv', header=False, index=False, sep='|')\n",
    "df_test.to_csv('test.csv', header=False, index=False, sep='|')\n",
    "\n",
    "#Liberar RAM\n",
    "del wiki, df_train, df_valid, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar datos con fastai.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm = TextLMDataBunch.from_csv(path = '', delimiter = '|', csv_name='train.csv', test='test.csv', \n",
    "          max_vocab=300000, min_freq=0)\n",
    "itos=data_lm.train_ds.vocab.itos\n",
    "vocab_size=len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens=[data_lm.train_ds[i][0].data for i in range(len(data_lm.train_ds))]\n",
    "test_tokens=[data_lm.valid_ds[i][0].data for i in range(len(data_lm.valid_ds))]\n",
    "\n",
    "train_dataset = LMDataset_GCNN(train_tokens)\n",
    "test_dataset = LMDataset_GCNN(test_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear DataLoader y Samplers\n",
    "\n",
    "Los siguientes pasos son muy importantes para poder entrenar el modelo.\n",
    "\n",
    "- El Sampler se encarga de generar las muestras de manera correcta en el DataLoader\n",
    "- El DataLoader se encarga de cargar bien los datos para poder manejarlos en memoria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Samplers\n",
    "train_sampler = SortishSampler_GCNN(data_length = len(train_dataset),key=lambda x:len(train_dataset[x][0]), bs = params['batch_size'])\n",
    "test_sampler = SortSampler_GCNN(test_dataset,key = lambda x:len(test_dataset[x][0]))\n",
    "\n",
    "#DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size = params['batch_size'], collate_fn=pad_collate_GCNN, sampler = train_sampler, pin_memory= False)\n",
    "test_loader = DataLoader(test_dataset, batch_size= params['batch_size'], collate_fn=pad_collate_GCNN, sampler = test_sampler, pin_memory= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embeddings(path, itos, embedding_size):\n",
    "    \"\"\" A partir del path de un .txt con embeddings genera un vector numpy de embeddings\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    word2idx = {}\n",
    "    vectors = []\n",
    "    idx = 0\n",
    "    with open(path, 'rb') as file:\n",
    "        for line in file:\n",
    "            line = line.decode().split()\n",
    "            word = line[0]\n",
    "            words.append(word)\n",
    "            word2idx[word] = idx\n",
    "            idx += 1\n",
    "            vectors.append(line[1:])\n",
    "    \n",
    "    #Verificamos que tengamos misma cantidad de palabras y vectores\n",
    "    assert len(words) == len(vectors)\n",
    "    \n",
    "    #Generamos diccionario\n",
    "    temporal_dictionary = collections.defaultdict(lambda:-1, {v:k for k,v in enumerate(words)})\n",
    "    unk_row = vectors[-1] # vector default para <unk>\n",
    "    #Creamos vector de tamaño número de palabras, por tamaño de embedding\n",
    "    embeddings = np.zeros((len(itos), embedding_size), dtype=np.float32)\n",
    "    \n",
    "    #Vamos iterando por palabras del diccionario que creamos antes para generar vector de embeddings con la misma\n",
    "    #correspondencia de indices\n",
    "    for i, word in enumerate(itos):\n",
    "        index = temporal_dictionary[word] #indice de palabra en diccionario temporal de embeddings cargados\n",
    "        #Generamos el vector de embeddings \n",
    "        embeddings[i] = vectors[index] if index>=0 else unk_row #Si no lo encuentra asigna vector por defecto \n",
    "        \n",
    "    return embeddings\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'glove.6B.300d.txt'\n",
    "embeddings = make_embeddings(path, itos, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Primero generamos el modelo, los párametros están en config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 322.00 MiB (GPU 0; 3.95 GiB total capacity; 2.52 GiB already allocated; 274.12 MiB free; 2.87 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-c94632a5a352>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Si se quiere usar GPU hay que poner el modelo en GPU con .cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'use_gpu'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mGCNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m#Setear los embeddings de glove en la capa de embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mGCNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \"\"\"\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \"\"\"\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 322.00 MiB (GPU 0; 3.95 GiB total capacity; 2.52 GiB already allocated; 274.12 MiB free; 2.87 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "GCNN = GCNNmodel(vocab_size, params['emb_sz'], params['k'], params['nh'], params['nl'], params['downbot'])\n",
    "#Si se quiere usar GPU hay que poner el modelo en GPU con .cuda()\n",
    "if params['use_gpu']:\n",
    "    GCNN.cuda()\n",
    "    #Setear los embeddings de glove en la capa de embeddings\n",
    "    GCNN.embed.weight.data = torch.FloatTensor(embeddings).cuda()\n",
    "else:\n",
    "    GCNN.model.embed.weight.data = torch.FloatTensor(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCNNmodel(\n",
       "  (embed): Embedding(280376, 300)\n",
       "  (inlayer): GLUblock(\n",
       "    (convresid): Conv2d(300, 600, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (leftpad): ConstantPad2d(padding=(0, 0, 3, 0), value=0)\n",
       "    (convx1a): Conv2d(300, 15, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (convx2a): Conv2d(300, 15, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (convx1b): Conv2d(15, 15, kernel_size=(4, 1), stride=(1, 1))\n",
       "    (convx2b): Conv2d(15, 15, kernel_size=(4, 1), stride=(1, 1))\n",
       "    (convx1c): Conv2d(15, 600, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (convx2c): Conv2d(15, 600, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (GLUlayers): Sequential(\n",
       "    (0): GLUblock(\n",
       "      (convresid): Conv2d(600, 600, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (leftpad): ConstantPad2d(padding=(0, 0, 3, 0), value=0)\n",
       "      (convx1a): Conv2d(600, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (convx2a): Conv2d(600, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (convx1b): Conv2d(30, 30, kernel_size=(4, 1), stride=(1, 1))\n",
       "      (convx2b): Conv2d(30, 30, kernel_size=(4, 1), stride=(1, 1))\n",
       "      (convx1c): Conv2d(30, 600, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (convx2c): Conv2d(30, 600, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (1): GLUblock(\n",
       "      (convresid): Conv2d(600, 600, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (leftpad): ConstantPad2d(padding=(0, 0, 3, 0), value=0)\n",
       "      (convx1a): Conv2d(600, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (convx2a): Conv2d(600, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (convx1b): Conv2d(30, 30, kernel_size=(4, 1), stride=(1, 1))\n",
       "      (convx2b): Conv2d(30, 30, kernel_size=(4, 1), stride=(1, 1))\n",
       "      (convx1c): Conv2d(30, 600, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (convx2c): Conv2d(30, 600, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (2): GLUblock(\n",
       "      (convresid): Conv2d(600, 600, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (leftpad): ConstantPad2d(padding=(0, 0, 3, 0), value=0)\n",
       "      (convx1a): Conv2d(600, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (convx2a): Conv2d(600, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (convx1b): Conv2d(30, 30, kernel_size=(4, 1), stride=(1, 1))\n",
       "      (convx2b): Conv2d(30, 30, kernel_size=(4, 1), stride=(1, 1))\n",
       "      (convx1c): Conv2d(30, 600, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (convx2c): Conv2d(30, 600, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (3): GLUblock(\n",
       "      (convresid): Conv2d(600, 600, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (leftpad): ConstantPad2d(padding=(0, 0, 3, 0), value=0)\n",
       "      (convx1a): Conv2d(600, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (convx2a): Conv2d(600, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (convx1b): Conv2d(30, 30, kernel_size=(4, 1), stride=(1, 1))\n",
       "      (convx2b): Conv2d(30, 30, kernel_size=(4, 1), stride=(1, 1))\n",
       "      (convx1c): Conv2d(30, 600, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (convx2c): Conv2d(30, 600, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (out): AdaptiveLogSoftmaxWithLoss(\n",
       "    (head): Linear(in_features=600, out_features=11217, bias=False)\n",
       "    (tail): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=600, out_features=150, bias=False)\n",
       "        (1): Linear(in_features=150, out_features=44860, bias=False)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=600, out_features=37, bias=False)\n",
       "        (1): Linear(in_features=37, out_features=224301, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-b6cadfecfa06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m#Actualizar párametros\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input."
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_loss_list = []; test_loss_list = []\n",
    "for epoch in range(params['epochs']):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        #Si queremos usar GPU hay que pasar los datos a GPU\n",
    "        if params['use_gpu']:\n",
    "            data.cuda()\n",
    "            \n",
    "        #Optimizer\n",
    "        if params['opttype'] == 'sgd':\n",
    "            optimizer = torch.optim.SGD(GCNN.parameters(), lr = params['lr'], momentum=params['mom'], weight_decay=params['wd'],\n",
    "                                       nesterov= params['nesterov'])\n",
    "        elif params['opttype'] == 'adam':\n",
    "            optimizer = torch.optim.Adam(GCNN.parameters(), lr = params['lr'], betas = (params['mom'], 0.999))\n",
    "        \n",
    "        #Dejar los gradientes del optimizador en 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        #Forward pass\n",
    "        output = GCNN(data)\n",
    "        \n",
    "        #Actualizar párametros\n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        train_loss_list.append(loss.item())\n",
    "        \n",
    "        #Gradient clipping\n",
    "        if params['grad_clip'] != 0:\n",
    "            nn.utils.clip_grad_value_(GCNN.parameters(), params['grad_clip'])\n",
    "            \n",
    "        #Actualizar pesos\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            elapsed_time=time.time()-start_time\n",
    "            print('Epoch: {}  Batches: {}  Loss: {}'.format(epoch, batch_idx, loss.item()))\n",
    "    \n",
    "    #Verificamos ahora en el test set\n",
    "    val_loss=[]\n",
    "    \n",
    "    #Gradientes no se actualizan y modelo en modo evaluacion\n",
    "    with torch.no_grad():\n",
    "        GCNN.eval()\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Forward\n",
    "            output = GCNN(data)\n",
    "            loss = output.loss\n",
    "            output.output\n",
    "            val_loss.append(loss.data.item())\n",
    "        \n",
    "    \n",
    "    #Modelo en modo entrenamiento\n",
    "    GCNN.train()\n",
    "    ave_val_loss=sum(val_loss) / len(val_loss)\n",
    "    val_update_string='Validation Loss: {:.4f}\\tPerp: {:.4f}'.format(ave_val_loss,np.exp(ave_val_loss))\n",
    "    print(val_update_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1440"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "72*20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
