{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "\n",
    "1. [Model](#1)\n",
    "2. [Implementation](#2)\n",
    "3. [WikiText 103 dataset processing](#3)\n",
    "4. [WikiText 103 Benchmark](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Model <a class=\"anchor\" id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un modelo de lenguaje busca calcular la probabilidad de observar una secuancia de tokens o palabras $w_{0}, \\ldots, w_{N}$, usando la regla de la cadena de probabilidades la probabilidad conjunta de observar tal secuencia se descompone en la siguiente expresión\n",
    ":\n",
    "\\begin{equation*}\n",
    "p(w_{0}, \\ldots, w_{N}) = \\prod_{i=0}^{N}p(w_{i}|\\ldots, w_{i-1})\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un modelo de N-grams posee una fuerte suposición sobre las probabilidades condicionales de una palabra y su historia ($p(w_{i}|\\ldots, w_{i-1})$), así la probabilidad de obserbar la palabra $w_{i}$ depende de los N-1 palabras anteriores. Por ejemplo:\n",
    "\n",
    "#### Unigrams\n",
    "\\begin{equation*}\n",
    "p(w_{i}|\\ldots, w_{i-1}) = p(w_{i})\n",
    "\\end{equation*}\n",
    "\n",
    "#### Bigrams\n",
    "\\begin{equation*}\n",
    "p(w_{i}|\\ldots, w_{i-1}) = p(w_{i}| w_{i-1})\n",
    "\\end{equation*}\n",
    "\n",
    "#### Trigrams\n",
    "\\begin{equation*}\n",
    "p(w_{i}|\\ldots, w_{i-1}) = p(w_{i}| w_{i-2}, w_{i-1})\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigrams with linear interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El baseline a implementar consiste de un modelo de trigramas con interpolación lineal para evitar el problema de **zero-count**, este problema ocurre cuando la probabilidad de observar un trigrama es cero, lo que trae por consecuencia que la probabilidad conjunta sea cero, pues usando regla de la cadena y markov de segundo orden la probabilidad conjunta se descompone en el producto de los trigramas de la secuencia y basta que un trigrama tenga probabilidad cero para hechar a perder el cálculo de la probabilidad.\n",
    "\n",
    "#### Trigrams\n",
    "\\begin{equation*}\n",
    "p(w_{0}, \\ldots, w_{N}) = \\prod_{i=0}^{N} p(w_{i}| w_{i-2}, w_{i-1})\n",
    "\\end{equation*}\n",
    "\n",
    "#### Trigrams with linear interpolation\n",
    "\n",
    "La probabilidad que entrega este modelo es la combinación convexa de las probabilidades que entregan los modelos unigrams, bigrams y trigrams.\n",
    "\n",
    "\\begin{align}\n",
    "\\nonumber\n",
    "& p(w_{0}, \\ldots, w_{N}) = \\prod_{i=0}^{N}\\lambda_{1} p(w_{i}| w_{i-2}, w_{i-1})+\\lambda_{2} p(w_{i}| w_{i-1})+\\lambda_{3} p(w_{i})\\\\ \\nonumber\n",
    "& \\lambda_{i}\\geq 0, \\; \\lambda_{1}+\\lambda_{2}+\\lambda_{3}=1\\ \\nonumber\n",
    "\\end{align}\n",
    "\n",
    "Los parámetros del modelo se estiman contando casos favorables versus totales, por ejemplo la probabilidad estimada de un trigrama viene dada por la siguiente expresión:\n",
    "\\begin{equation*}\n",
    "q(w_{i}|w_{i-2}, w_{i}) = \\frac{Count(w_{i-2}, w_{i-1}, w_{i})}{Count(w_{i-2}, w_{i-1})}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "### Perplexity\n",
    "\n",
    "\n",
    "La likelihood de un language model:\n",
    "\n",
    "\\begin{equation*}\n",
    "L = \\prod_{d=1}^{D}p(w_{1}, \\ldots, w_{n_{d}}) = \\prod_{d=1}^{D}\\prod_{i=1}^{n_{d}}p(w_{i}|\\ldots, w_{i-1})\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "Sea $D$ el número de documentos, $n_{d}$ el número de tokens del documento $d$, notar que los documentos son independientes.\n",
    "\n",
    "Luego, la negative log-likelihood:\n",
    "\n",
    "\\begin{equation*}\n",
    "NLL = \\sum_{d=1}^{D}\\sum_{i=1}^{n_{d}}-ln\\big(p(w_{i}|\\ldots, w_{i-1})\\big) = \\sum_{i=1}^{N}-ln\\big(p(w_{i}|\\ldots, w_{i-1})\\big)\n",
    "\\end{equation*}\n",
    "\n",
    "Donde, $N$ es el número de tokens presentes en el corpus.\n",
    "\n",
    "\\begin{equation*}\n",
    "perplexity = e^{\\frac{1}{N}\\times NLL}\n",
    "\\end{equation*}\n",
    "\n",
    "Por tanto, minimizar la perplexity es equivalente a minimizar NLL o maximizar LL.\n",
    "\n",
    "En el caso del modelo de trigramas la perplexity queda:\n",
    "\n",
    "\\begin{equation*}\n",
    "perplexity = exp\\left(\\frac{1}{N}\\sum_{i=1}^{N}-ln\\big(\\lambda_{1} q(w_{i}| w_{i-2}, w_{i-1})+\\lambda_{2} q(w_{i}| w_{i-1})+\\lambda_{3} q(w_{i})\\big)\\right)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementation <a class=\"anchor\" id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T14:06:07.619615Z",
     "start_time": "2019-10-10T14:06:07.615626Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "from scipy.stats import dirichlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trigrams():     \n",
    "\n",
    "    def fit(self, corpus):\n",
    "        \"\"\"\n",
    "        Ajusta el modelo en el corpus de entrenamiento, guarda los parámetros en atributos.\n",
    "\n",
    "        Parameters:\n",
    "            corpus: {list of str}, shape (corpus_size) \n",
    "                corpus de entrenamiento tokenizado.\n",
    "        Returns:\n",
    "            self: object\n",
    "        \"\"\"\n",
    "        #Número de documentos\n",
    "        N = len(corpus) \n",
    "       \n",
    "        ##Unigrams\n",
    "        model1 = defaultdict(lambda: 0)\n",
    "        #contar unigramas\n",
    "        for doc in corpus:\n",
    "            doc = tokenizer(doc)\n",
    "            for word in doc:\n",
    "                model1[word]+=1\n",
    "        n_tokens = np.array(list(model1.values())).sum()\n",
    "        vocabulary = list(model1.keys())\n",
    "        #probabilidades unigramas: dividir la frecuencia por el total de tokens\n",
    "        for word in vocabulary:\n",
    "            model1[word] = model1[word]/n_tokens\n",
    "        model1['</s>']= N/n_tokens\n",
    "        \n",
    "        \n",
    "        ##Bigrams\n",
    "        model2 = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        #probabiliades bigramas\n",
    "        for doc in corpus:\n",
    "            doc = tokenizer(doc)\n",
    "            doc = ['<s>']+doc+['</s>']#start and end symbol\n",
    "            n = len(doc)#largo del documento\n",
    "            for i in range(1, n):   \n",
    "                word_i_1 = doc[i-1]\n",
    "                word_i = doc[i]\n",
    "                if i == 1:#primera palabra\n",
    "                    q_w1 = N\n",
    "                else:#segunda palabra en adelante\n",
    "                    q_w1 = model1[word_i_1]*n_tokens\n",
    "                \n",
    "                model2[word_i_1][word_i]+=1/q_w1\n",
    "\n",
    "        ##Trigrams\n",
    "        model3 = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0)))\n",
    "        #probabiliades trigramas\n",
    "        for doc in corpus:\n",
    "            doc = tokenizer(doc)\n",
    "            doc = ['<s>', '<s>']+doc+['</s>']#start and end symbol\n",
    "            n = len(doc) #largo del documento\n",
    "            for i in range(2, n):\n",
    "                word_i_2 = doc[i-2]\n",
    "                word_i_1 = doc[i-1]\n",
    "                word_i = doc[i]\n",
    "                if i == 2:#primera palabra\n",
    "                    q_w1_w2 = N\n",
    "                elif i == 3:#segunda palabra\n",
    "                    q_w2 = N\n",
    "                    q_w1_w_2 = model2[word_i_2][word_i_1]*q_w2\n",
    "                else:#tercera palabra en adelante\n",
    "                    q_w2 = model1[word_i_2]*n_tokens\n",
    "                    q_w1_w2 = model2[word_i_2][word_i_1]*q_w2\n",
    "                    \n",
    "                model3[word_i_2][word_i_1][word_i]+=1/q_w1_w2\n",
    "        \n",
    "        #guardar parámetros en atributos y el vocabulario\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.model3 = model3\n",
    "        self.vocabulary = vocabulary\n",
    "\n",
    "    def predict_proba_trigam(self, lamb, trigram):\n",
    "        \"\"\"\n",
    "        Calcula la probabilidad de un trigrama usando interpolación lineal.\n",
    "            - p(w3|w1,w2) = lamb_1*q(w3|w1,w2)+lamb_2*q(w3|w2)+lamb_3*q(w3)\n",
    "            - lamb>=0, lamb_1+lamb_2+lamb_3=1\n",
    "        Parameters:\n",
    "            lamb: array, shape = 3\n",
    "                hiperparámetros del modelo, cada componente debe ser positivo y deben sumar 1.\n",
    "            trigram: tuple of str, shape = 3\n",
    "                trigrama, de la forma (w1, w2, w3), donde w3 es el último token de la secuencia.\n",
    "\n",
    "        Returns:\n",
    "            score: float\n",
    "                probabilidad del trigrama\n",
    "        \"\"\"\n",
    "        w1, w2, w3 = trigram\n",
    "        proba = lamb[0]*self.model3[w1][w2][w3]+lamb[1]*self.model2[w2][w3]+lamb[2]*self.model1[w3]\n",
    "        return proba\n",
    "        \n",
    "    def get_perplexity(self, lamb, corpus):\n",
    "\n",
    "        \"\"\"\n",
    "        Obtiene la perplexity del modelo en un conjunto de validación\n",
    "            -perplexity = exp(NLL/N), donde NLL es la negative-log-likelihood del modelo y \n",
    "             N el número de trigramas en el corpus.\n",
    "\n",
    "        Parameters:\n",
    "            - corpus: {list of list of str}, shape (corpus_size) \n",
    "                  corpus de validación tokenizado.\n",
    "            - lamb: {array}, shape =3\n",
    "                  hiperparámetros del modelo, cada componente debe ser positivo y deben sumar 1.\n",
    "        Returns:\n",
    "            score: float\n",
    "                perplexity\n",
    "        \"\"\"\n",
    "\n",
    "        N = 0 #contador de trigramas del conjunto de validación\n",
    "        NLL = 0 #negative-log-likelihood\n",
    "        for doc in corpus:\n",
    "            doc = ['<s>', '<s>']+doc+['</s>']#start and end symbol\n",
    "            n = len(doc) #largo del documento\n",
    "            for i in range(2, n):\n",
    "                word_i_2 = doc[i-2]\n",
    "                word_i_1 = doc[i-1]\n",
    "                word_i = doc[i]\n",
    "                trigram = word_i_2, word_i_1, word_i, \n",
    "                trigram_proba = self.predict_proba_trigam(lamb, trigram)\n",
    "                NLL = NLL - np.log(trigram_proba) \n",
    "                N = N+1\n",
    "        perplexity = np.exp(NLL/N)\n",
    "\n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. WikiText 103 dataset processing <a class=\"anchor\" id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos los archivos a partir de un .zip, el .zip cuenta con tres archivos planos, train, valid y test. De cada archivo se extraen los documentos y son guardados en una lista dentro de un diccionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(dataset):\n",
    "    vocabulary = set()\n",
    "    for doc in dataset:\n",
    "        set_doc = set(doc.split())\n",
    "        vocabulary.update(set_doc)\n",
    "    return vocabulary\n",
    "\n",
    "def tokenizer(doc, vocabulary=None):\n",
    "    tokens = doc.split()\n",
    "    if vocabulary:\n",
    "        tokens = [word for word in tokens if word in vocabulary]\n",
    "    return tokens\n",
    "\n",
    "def remove_out_of_vocabulary(vocabulary, dataset):\n",
    "    new_dataset = []\n",
    "    deleted = 0\n",
    "    \n",
    "    for doc in dataset:\n",
    "        tokens =  tokenizer(doc, vocabulary)\n",
    "        new_dataset.append(tokens)\n",
    "    \n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importar datasets\n",
    "shutil.unpack_archive('../wikitext-103-v1.zip', extract_dir='dataset')\n",
    "working_dir = os.path.join(os.getcwd(), 'dataset', 'wikitext-103')\n",
    "wikitext_files = os.listdir(working_dir)\n",
    "\n",
    "wiki = {}\n",
    "for wikitext_file in wikitext_files:\n",
    "    with open(os.path.join(working_dir, wikitext_file), encoding='utf-8') as data_file:\n",
    "        name = wikitext_file.split('.')[1]\n",
    "        corpus = []\n",
    "        for index, line in enumerate(data_file):\n",
    "            # filtrar lineas vacías y headers\n",
    "            if len(line) < 3 or line[1] == '=':\n",
    "                continue\n",
    "            else:\n",
    "                corpus.append(line.strip())\n",
    "        #list of str: dataset no tokenizado        \n",
    "        wiki[name] = corpus\n",
    "shutil.rmtree('dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He had a recurring role in 2003 on two episodes of The Bill , as character \" Connor Price \" . In 2004 Boulter landed a role as \" Craig \" in the episode \" Teddy \\'s Story \" of the television series The Long Firm ; he starred alongside actors Mark Strong and Derek Jacobi . Boulter starred as \" Darren \" , in the 2005 theatre productions of the Philip Ridley play Mercury Fur . It was performed at the Drum Theatre in Plymouth , and the <unk> Chocolate Factory in London . He was directed by John Tiffany and starred alongside Ben Whishaw , Shane Zaza , Harry Kent , Fraser Ayres , Sophie Stanton and Dominic Hall . Boulter received a favorable review in The Daily Telegraph : \" The acting is shatteringly intense , with wired performances from Ben Whishaw ( now unrecognisable from his performance as Trevor Nunn \\'s Hamlet ) , Robert Boulter , Shane Zaza and Fraser Ayres . \" The Guardian noted , \" Ben Whishaw and Robert Boulter offer tenderness amid the savagery . \"'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki['test'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "859532"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wiki['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extraemos el vocabulario de train y removemos los tokens de validation y test que no estan en él\n",
    "vocabulary = get_vocabulary(wiki['train'])\n",
    "wiki['valid'] = remove_out_of_vocabulary(vocabulary, wiki['valid']) \n",
    "wiki['test'] = remove_out_of_vocabulary(vocabulary, wiki['test']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. WikiText 103 Benchmark <a class=\"anchor\" id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo de trigramas en el conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T14:16:35.648966Z",
     "start_time": "2019-10-10T14:08:40.855715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 28s, sys: 5.02 s, total: 8min 34s\n",
      "Wall time: 8min 33s\n"
     ]
    }
   ],
   "source": [
    "wikitext_model = Trigrams()\n",
    "%time wikitext_model.fit(wiki['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluamos el desempeño del modelo de trigramas en validation para 100 configuraciones diferentes de $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 108.43068838119507\n"
     ]
    }
   ],
   "source": [
    "ti = time()\n",
    "#una muestra que distribuye dirichlet es un vector donde cada componente es no negativo y suman 1 \n",
    "pdf = dirichlet(alpha = [64, 16, 4]) #muestras que tienden a satisfacer lambda1>lambda2>lambda3\n",
    "lambdas = pdf.rvs(size=100, random_state=0) #generar 100 muestras\n",
    "#computar perplexity para las 100 configuraciones en validación\n",
    "perplexity = []\n",
    "for i in range(len(lambdas)):\n",
    "    perplexity.append(wikitext_model.get_perplexity(lambdas[i], wiki['valid']))\n",
    "tf = time()\n",
    "print(f\"Wall time: {tf-ti}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-10T14:06:06.655Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambda</th>\n",
       "      <th>perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[0.6155053151143111, 0.2494869060497376, 0.135...</td>\n",
       "      <td>200.398335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>[0.6389284514853635, 0.27312456407959157, 0.08...</td>\n",
       "      <td>200.978871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>[0.662545908458533, 0.24844956753710032, 0.089...</td>\n",
       "      <td>203.184069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>[0.693068868830631, 0.23634674049083365, 0.070...</td>\n",
       "      <td>206.868809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>[0.6881357187938214, 0.20199917444681775, 0.10...</td>\n",
       "      <td>206.879514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               lambda  perplexity\n",
       "4   [0.6155053151143111, 0.2494869060497376, 0.135...  200.398335\n",
       "5   [0.6389284514853635, 0.27312456407959157, 0.08...  200.978871\n",
       "95  [0.662545908458533, 0.24844956753710032, 0.089...  203.184069\n",
       "12  [0.693068868830631, 0.23634674049083365, 0.070...  206.868809\n",
       "92  [0.6881357187938214, 0.20199917444681775, 0.10...  206.879514"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_scores = pd.DataFrame({'lambda':pd.Series(lambdas.tolist()), 'perplexity':pd.Series(perplexity)})\n",
    "validation_scores.sort_values('perplexity').head() #mostramos las primeras 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomamos la configuración con mejor perplexity (el valor mínimo) y evaluamos en test obteniendose una perplexity de 205.44."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-10T14:06:06.839Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambda</th>\n",
       "      <th>perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[0.6155053151143111, 0.2494869060497376, 0.135...</td>\n",
       "      <td>200.398335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>[0.6389284514853635, 0.27312456407959157, 0.08...</td>\n",
       "      <td>200.978871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>[0.662545908458533, 0.24844956753710032, 0.089...</td>\n",
       "      <td>203.184069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>[0.693068868830631, 0.23634674049083365, 0.070...</td>\n",
       "      <td>206.868809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>[0.6881357187938214, 0.20199917444681775, 0.10...</td>\n",
       "      <td>206.879514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               lambda  perplexity\n",
       "4   [0.6155053151143111, 0.2494869060497376, 0.135...  200.398335\n",
       "5   [0.6389284514853635, 0.27312456407959157, 0.08...  200.978871\n",
       "95  [0.662545908458533, 0.24844956753710032, 0.089...  203.184069\n",
       "12  [0.693068868830631, 0.23634674049083365, 0.070...  206.868809\n",
       "92  [0.6881357187938214, 0.20199917444681775, 0.10...  206.879514"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_configurations = validation_scores.sort_values('perplexity', ascending=True).head()\n",
    "best_lambda = best_configurations['lambda'].iloc[0]\n",
    "best_configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-10T14:06:06.843Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205.44248549841095"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity_test = wikitext_model.get_perplexity(best_lambda, wiki['test'])\n",
    "perplexity_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
