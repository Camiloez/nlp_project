{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Language-Modeling\" data-toc-modified-id=\"Language-Modeling-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Language Modeling</a></span></li><li><span><a href=\"#N-grams\" data-toc-modified-id=\"N-grams-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>N-grams</a></span><ul class=\"toc-item\"><li><span><a href=\"#Unigrams\" data-toc-modified-id=\"Unigrams-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Unigrams</a></span></li><li><span><a href=\"#Bigrams\" data-toc-modified-id=\"Bigrams-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Bigrams</a></span></li><li><span><a href=\"#Trigrams\" data-toc-modified-id=\"Trigrams-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>Trigrams</a></span></li></ul></li><li><span><a href=\"#Trigrams-with-linear-interpolation\" data-toc-modified-id=\"Trigrams-with-linear-interpolation-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Trigrams with linear interpolation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Trigrams\" data-toc-modified-id=\"Trigrams-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Trigrams</a></span></li><li><span><a href=\"#Trigrams-with-linear-interpolation\" data-toc-modified-id=\"Trigrams-with-linear-interpolation-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Trigrams with linear interpolation</a></span></li></ul></li></ul></li><li><span><a href=\"#Implementation\" data-toc-modified-id=\"Implementation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Implementation</a></span></li><li><span><a href=\"#WikiText-103-dataset-processing\" data-toc-modified-id=\"WikiText-103-dataset-processing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>WikiText 103 dataset processing</a></span></li><li><span><a href=\"#WikiText-103-Benchmark\" data-toc-modified-id=\"WikiText-103-Benchmark-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>WikiText 103 Benchmark</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un modelo de lenguaje busca calcular la probabilidad de observar una secuancia de tokens o palabras $w_{0}, \\ldots, w_{N}$, usando la regla de la cadena de probabilidades la probabilidad conjunta de observar tal secuencia se descompone en la siguiente expresión\n",
    ":\n",
    "\\begin{equation*}\n",
    "p(w_{0}, \\ldots, w_{N}) = \\prod_{i=0}^{N}p(w_{i}|\\ldots, w_{i-1})\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un modelo de N-grams posee una fuerte suposición sobre las probabilidades condicionales de una palabra y su historia ($p(w_{i}|\\ldots, w_{i-1})$), así la probabilidad de obserbar la palabra $w_{i}$ depende de los N-1 palabras anteriores. Por ejemplo:\n",
    "\n",
    "#### Unigrams\n",
    "\\begin{equation*}\n",
    "p(w_{i}|\\ldots, w_{i-1}) = p(w_{i})\n",
    "\\end{equation*}\n",
    "\n",
    "#### Bigrams\n",
    "\\begin{equation*}\n",
    "p(w_{i}|\\ldots, w_{i-1}) = p(w_{i}| w_{i-1})\n",
    "\\end{equation*}\n",
    "\n",
    "#### Trigrams\n",
    "\\begin{equation*}\n",
    "p(w_{i}|\\ldots, w_{i-1}) = p(w_{i}| w_{i-2}, w_{i-1})\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigrams with linear interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El baseline a implementar consiste de un modelo de trigramas con interpolación lineal para evitar el problema de **zero-count**, este problema ocurre cuando la probabilidad de observar un trigrama es cero, lo que trae por consecuencia que la probabilidad conjunta sea cero, pues usando regla de la cadena y markov de segundo orden la probabilidad conjunta se descompone en el producto de los trigramas de la secuencia y basta que un trigrama tenga probabilidad cero para hechar a perder el cálculo de la probabilidad.\n",
    "\n",
    "#### Trigrams\n",
    "\\begin{equation*}\n",
    "p(w_{0}, \\ldots, w_{N}) = \\prod_{i=0}^{N} p(w_{i}| w_{i-2}, w_{i-1})\n",
    "\\end{equation*}\n",
    "\n",
    "#### Trigrams with linear interpolation\n",
    "\n",
    "La probabilidad que entrega este modelo es la combinación convexa de las probabilidades que entregan los modelos unigrams, bigrams y trigrams.\n",
    "\n",
    "\\begin{align}\n",
    "\\nonumber\n",
    "& p(w_{0}, \\ldots, w_{N}) = \\prod_{i=0}^{N}\\lambda_{1} p(w_{i}| w_{i-2}, w_{i-1})+\\lambda_{2} p(w_{i}| w_{i-1})+\\lambda_{3} p(w_{i})\\\\ \\nonumber\n",
    "& \\lambda_{i}\\geq 0, \\; \\lambda_{1}+\\lambda_{2}+\\lambda_{3}=1\\ \\nonumber\n",
    "\\end{align}\n",
    "\n",
    "Los parámetros del modelo se estiman contando casos favorables versus totales, por ejemplo la probabilidad estimada de un trigrama viene dada por la siguiente expresión:\n",
    "\\begin{equation*}\n",
    "q(w_{i}|w_{i-2}, w_{i}) = \\frac{Count(w_{i-2}, w_{i-1}, w_{i})}{Count(w_{i-2}, w_{i-1})}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T14:06:07.613631Z",
     "start_time": "2019-10-10T14:06:05.938691Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import nltk\n",
    "nltk.download('reuters', quiet=True)\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T14:06:07.619615Z",
     "start_time": "2019-10-10T14:06:07.615626Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import reuters\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T14:06:07.640564Z",
     "start_time": "2019-10-10T14:06:07.622607Z"
    }
   },
   "outputs": [],
   "source": [
    "class Trigrams():     \n",
    "    def fit(self, corpus):\n",
    "        \"\"\"\n",
    "        Ajusta el modelo en el corpus de entrenamiento, guarda los parámetros en atributos.\n",
    "\n",
    "        Parameters:\n",
    "            corpus: {list of str}, shape (corpus_size) \n",
    "            corpus de entrenamiento tokenizado.\n",
    "        Returns:\n",
    "            self: object\n",
    "        \"\"\"\n",
    "        ## Crear diccionarios que guardaran la probabilidad de cada n-gram\n",
    "        model1 = defaultdict(lambda: 0)\n",
    "        model2 = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        model3 = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "        ## Contar frecuencia de co-ocurrencia\n",
    "        N = len(corpus)\n",
    "        for sentence in corpus:\n",
    "            # Unigrams\n",
    "            model1[None]=N\n",
    "            for w1 in sentence:\n",
    "                model1[w1]+=1\n",
    "            # Bigrams\n",
    "            model2[None][None]=N\n",
    "            for w1, w2 in bigrams(sentence, pad_right=True, pad_left=True):\n",
    "                model2[w1][w2] += 1\n",
    "            # Trigrams\n",
    "            for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "                model3[(w1, w2)][w3] += 1\n",
    "\n",
    "        ## Transformar conteo a probabilidades\n",
    "        # Trigrams\n",
    "        for w1_w2 in model3:\n",
    "            w1, w2 = w1_w2[0], w1_w2[1]\n",
    "            total_count = model2[w1][w2]\n",
    "            for w3 in model3[w1_w2]:\n",
    "                model3[w1_w2][w3] /= total_count\n",
    "        # Bigrams        \n",
    "        for w1 in model2:\n",
    "            total_count = model1[w1]\n",
    "            for w2 in model2[w1]:\n",
    "                model2[w1][w2] /= total_count\n",
    "        # Unigrams    \n",
    "        total_count = sum(model1.values())\n",
    "        for w1 in model1:\n",
    "            model1[w1] /= total_count\n",
    "\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.model3 = model3     \n",
    "        \n",
    "    def predict_proba_trigam(self, lamb, trigram):\n",
    "        \"\"\"\n",
    "        Calcula la probabilidad de un trigrama usando interpolación lineal.\n",
    "            - p(w3|w1,w2) = lamb_1*q(w3|w1,w2)+lamb_2*q(w3|w2)+lamb_3*q(w3)\n",
    "            - lamb>=0, lamb_1+lamb_2+lamb_3=1\n",
    "        Parameters:\n",
    "            lamb: {array}, shape =3\n",
    "            hiperparámetros del modelo, cada componente debe ser positivo y deben sumar 1.\n",
    "\n",
    "        Returns:\n",
    "            score: float\n",
    "            probabilidad del trigrama\n",
    "        \"\"\"\n",
    "        w1, w2, w3 = trigram\n",
    "        proba = lamb[0]*self.model3[(w1,w2)][w3]+lamb[1]*self.model2[w2][w3]+lamb[2]*self.model1[w3]\n",
    "        return proba\n",
    "        \n",
    "    def get_perplexity(self, lamb, corpus):\n",
    "\n",
    "        \"\"\"\n",
    "        Obtiene la perplexity del modelo en un conjunto de validación\n",
    "            -perplexity = exp(NLL/N), donde NLL es la negative-log-likelihood del modelo y \n",
    "             N el número de trigramas en el corpus.\n",
    "\n",
    "        Parameters:\n",
    "            - corpus: {list of str}, shape (corpus_size) \n",
    "              corpus de validación tokenizado.\n",
    "            - lamb: {array}, shape =3\n",
    "              hiperparámetros del modelo, cada componente debe ser positivo y deben sumar 1.\n",
    "        Returns:\n",
    "            score: float\n",
    "            perplexity\n",
    "        \"\"\"\n",
    "\n",
    "        N = 0 #contador de trigramas del conjunto de validación\n",
    "        NLL = 0 #negative-log-likelihood\n",
    "        for sentence in corpus:\n",
    "            for trigram in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "                trigram_proba = self.predict_proba_trigam(lamb, trigram)\n",
    "                NLL = NLL - np.log(trigram_proba) \n",
    "                N = N+1\n",
    "        perplexity = np.exp(NLL/N)\n",
    "\n",
    "        return perplexity\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T14:06:16.402829Z",
     "start_time": "2019-10-10T14:06:07.644549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', '.', 'S', '.', 'And', 'Japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'Asia', \"'\", 's', 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far', '-', 'reaching', 'economic', 'damage', ',', 'businessmen', 'and', 'officials', 'said', '.']\n",
      "54716\n"
     ]
    }
   ],
   "source": [
    "corpus = reuters.sents()\n",
    "print(corpus[0])\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T14:06:31.174168Z",
     "start_time": "2019-10-10T14:06:16.404824Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "model = Trigrams()\n",
    "%time model.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T14:06:46.056056Z",
     "start_time": "2019-10-10T14:06:31.176128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.383050732528654"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lamb = [0.7, 0.2, 0.1]\n",
    "%time model.get_perplexity(lamb, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T14:06:46.070018Z",
     "start_time": "2019-10-10T14:06:46.058051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ASIAN', 'EXPORTERS', 'FEAR')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7043479387228446"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calcular probabilidad de un trigrama\n",
    "example = trigrams(corpus[0], pad_right=True, pad_left=True)\n",
    "example_trigrams = []\n",
    "for trigram in example:\n",
    "    example_trigrams.append(trigram)\n",
    "    \n",
    "print(example_trigrams[2])\n",
    "model.predict_proba_trigam(lamb, example_trigrams[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WikiText 103 dataset processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the files were already tokenize we must build the tokens array for each dataset. Using the spaces left by the tokenization and the file names the three arrays of tokens are built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T14:07:15.513541Z",
     "start_time": "2019-10-10T14:06:46.075005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraphs on train:      859532\n",
      "Paragraphs on test:       2183\n",
      "Paragraphs on validation: 1841\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "shutil.unpack_archive('../wikitext-103-v1.zip', extract_dir='dataset')\n",
    "working_dir = os.path.join(os.getcwd(), 'dataset', 'wikitext-103')\n",
    "\n",
    "wikitext_files = os.listdir(working_dir)\n",
    "\n",
    "wiki_train = []\n",
    "wiki_train_vocabulary = []\n",
    "wiki_test = []\n",
    "wiki_valid = []\n",
    "\n",
    "for wikitext_file in wikitext_files:\n",
    "    with open(os.path.join(working_dir, wikitext_file), encoding='utf-8') as data_file:\n",
    "        for index, line in enumerate(data_file):\n",
    "            # Filter out empty lines and headers\n",
    "            if len(line) < 3 or line[1] == '=':\n",
    "                continue\n",
    "            \n",
    "            else:\n",
    "                if re.match(r'.*\\.train\\..*', wikitext_file):\n",
    "                    wiki_train.append(line.strip().split(' '))\n",
    "                    wiki_train_vocabulary.append(line.strip())\n",
    "                \n",
    "                elif re.match(r'.*\\.test\\..*', wikitext_file):\n",
    "                    wiki_test.append(line.strip().split(' '))\n",
    "                    \n",
    "                elif re.match(r'.*\\.valid\\..*', wikitext_file):\n",
    "                    wiki_valid.append(line.strip().split(' '))\n",
    "                    \n",
    "shutil.rmtree('dataset')\n",
    "                    \n",
    "print(f'Paragraphs on train:      {len(wiki_train)}')\n",
    "print(f'Paragraphs on test:       {len(wiki_test)}')\n",
    "print(f'Paragraphs on validation: {len(wiki_valid)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T14:08:40.853733Z",
     "start_time": "2019-10-10T14:07:15.515535Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 25s\n",
      "Vocabulary has 225785 tokens\n",
      "79926 words were out of vocabulary\n",
      "Wall time: 124 ms\n",
      "72275 words were out of vocabulary\n",
      "Wall time: 116 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_vocabulary(dataset):\n",
    "    vocabulary = []\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    \n",
    "    vectorizer.fit_transform(dataset)\n",
    "    vectorizer._validate_vocabulary()\n",
    "    \n",
    "    vocabulary = vectorizer.get_feature_names()\n",
    "\n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "def remove_out_of_vocabulary(vocabulary, dataset):\n",
    "    new_dataset = []\n",
    "    deleted = 0\n",
    "    \n",
    "    vocabulary = set(vocabulary)\n",
    "    \n",
    "    for paragraph in dataset:\n",
    "        new_paragraph = []\n",
    "        inter = set(paragraph) & vocabulary\n",
    "        \n",
    "        for word in paragraph:\n",
    "            if word in inter:\n",
    "                new_paragraph.append(word)\n",
    "            \n",
    "            else:\n",
    "                deleted += 1\n",
    "\n",
    "        new_dataset.append(new_paragraph)\n",
    "\n",
    "    print(f'{deleted} words were out of vocabulary')\n",
    "    return new_dataset\n",
    "\n",
    "%time vocabulary = get_vocabulary(wiki_train_vocabulary)\n",
    "\n",
    "print(f'Vocabulary has {len(vocabulary)} tokens')\n",
    "\n",
    "%time wiki_test = remove_out_of_vocabulary(vocabulary, wiki_test)\n",
    "%time wiki_valid = remove_out_of_vocabulary(vocabulary, wiki_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WikiText 103 Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must train a new model. This process is similar to the one shown in section 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T14:16:35.648966Z",
     "start_time": "2019-10-10T14:08:40.855715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 54s\n"
     ]
    }
   ],
   "source": [
    "wikitext_model = Trigrams()\n",
    "%time wikitext_model.fit(wiki_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-10T14:06:06.649Z"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import arange\n",
    "\n",
    "results = {}\n",
    "\n",
    "for lambda1 in arange(0, 1, 0.1):\n",
    "    for lambda2 in arange(0, 1, 0.1):\n",
    "        for lambda3 in arange(0, 1, 0.1):\n",
    "            wikitext_lamb = [\n",
    "                round(lambda1, 1), \n",
    "                round(lambda2, 1), \n",
    "                round(lambda3, 1)\n",
    "            ]\n",
    "            \n",
    "            if sum(wikitext_lamb) != 1.0:\n",
    "                continue\n",
    "            \n",
    "            results[str(wikitext_lamb)] = wikitext_model.get_perplexity(wikitext_lamb, wiki_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-10T14:06:06.655Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "def results_as_table():\n",
    "    rows = []\n",
    "\n",
    "    for key, value in results.items():\n",
    "        row = {\n",
    "            'configuration': key,\n",
    "            'perplexity': value\n",
    "        }\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pandas.DataFrame(rows)\n",
    "    \n",
    "    return df\n",
    "\n",
    "results_as_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the top 5 perplexity configurations and we run them with the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-10T14:06:06.839Z"
    }
   },
   "outputs": [],
   "source": [
    "df = results_as_table()\n",
    "best_configurations = df.sort_values('perplexity', ascending=True).head()\n",
    "best_configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-10T14:06:06.843Z"
    }
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "validation_configurations = best_configurations['configuration'].tolist()\n",
    "validation_configurations = list(map(lambda config : ast.literal_eval(config), validation_configurations))\n",
    "\n",
    "results = {}\n",
    "\n",
    "for config in validation_configurations:\n",
    "    results[str(config)] = wikitext_model.get_perplexity(config, wiki_valid)\n",
    "    \n",
    "results_as_table()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
