{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Model\n",
    "\n",
    "## 1.1 Language Modeling\n",
    "\n",
    "Un modelo de lenguaje busca calcular la probabilidad de observar una secuancia de tokens o palabras $w_{0}, \\ldots, w_{N}$, usando la regla de la cadena de probabilidades la probabilidad conjunta de observar tal secuencia se descompone en la siguiente expresión\n",
    ":\n",
    "\\begin{equation*}\n",
    "p(w_{0}, \\ldots, w_{N}) = \\prod_{i=0}^{N}p(w_{i}|\\ldots, w_{i-1})\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "## 1.2 N-grams \n",
    "Un modelo de N-grams posee una fuerte suposición sobre las probabilidades condicionales de una palabra y su historia ($p(w_{i}|\\ldots, w_{i-1})$), así la probabilidad de obserbar la palabra $w_{i}$ depende de los N-1 palabras anteriores. Por ejemplo:\n",
    "\n",
    "### Unigrams\n",
    "\\begin{equation*}\n",
    "p(w_{i}|\\ldots, w_{i-1}) = p(w_{i})\n",
    "\\end{equation*}\n",
    "\n",
    "### Bigrams\n",
    "\\begin{equation*}\n",
    "p(w_{i}|\\ldots, w_{i-1}) = p(w_{i}| w_{i-1})\n",
    "\\end{equation*}\n",
    "\n",
    "### Trigrams\n",
    "\\begin{equation*}\n",
    "p(w_{i}|\\ldots, w_{i-1}) = p(w_{i}| w_{i-2}, w_{i-1})\n",
    "\\end{equation*}\n",
    "\n",
    "## 1.3 Trigrams with linear interpolation\n",
    "\n",
    "El baseline a implementar consiste de un modelo de trigramas con interpolación lineal para evitar el problema de **zero-count**, este problema ocurre cuando la probabilidad de observar un trigrama es cero, lo que trae por consecuencia que la probabilidad conjunta sea cero, pues usando regla de la cadena y markov de segundo orden la probabilidad conjunta se descompone en el producto de los trigramas de la secuencia y basta que un trigrama tenga probabilidad cero para hechar a perder el cálculo de la probabilidad.\n",
    "\n",
    "### Trigrams\n",
    "\\begin{equation*}\n",
    "p(w_{0}, \\ldots, w_{N}) = \\prod_{i=0}^{N} p(w_{i}| w_{i-2}, w_{i-1})\n",
    "\\end{equation*}\n",
    "\n",
    "### Trigrams with linear interpolation\n",
    "\n",
    "La probabilidad que entrega este modelo es la combinación convexa de las probabilidades que entregan los modelos unigrams, bigrams y trigrams.\n",
    "\n",
    "\\begin{align}\n",
    "\\nonumber\n",
    "& p(w_{0}, \\ldots, w_{N}) = \\prod_{i=0}^{N}\\lambda_{1} p(w_{i}| w_{i-2}, w_{i-1})+\\lambda_{2} p(w_{i}| w_{i-1})+\\lambda_{3} p(w_{i})\\\\ \\nonumber\n",
    "& \\lambda_{i}\\geq 0, \\; \\lambda_{1}+\\lambda_{2}+\\lambda_{3}=1\\ \\nonumber\n",
    "\\end{align}\n",
    "\n",
    "Los parámetros del modelo se estiman contando casos favorables versus totales, por ejemplo la probabilidad estimada de un trigrama viene dada por la siguiente expresión:\n",
    "\\begin{equation*}\n",
    "q(w_{i}|w_{i-2}, w_{i}) = \\frac{Count(w_{i-2}, w_{i-1}, w_{i})}{Count(w_{i-2}, w_{i-1})}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import reuters\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trigrams():     \n",
    "    def fit(self, corpus):\n",
    "        \"\"\"\n",
    "        Ajusta el modelo en el corpus de entrenamiento, guarda los parámetros en atributos.\n",
    "\n",
    "        Parameters:\n",
    "            corpus: {list of str}, shape (corpus_size) \n",
    "            corpus de entrenamiento tokenizado.\n",
    "        Returns:\n",
    "            self: object\n",
    "        \"\"\"\n",
    "        ## Crear diccionarios que guardaran la probabilidad de cada n-gram\n",
    "        model1 = defaultdict(lambda: 0)\n",
    "        model2 = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        model3 = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "        ## Contar frecuencia de co-ocurrencia\n",
    "        N = len(corpus)\n",
    "        for sentence in corpus:\n",
    "            # Unigrams\n",
    "            model1[None]=N\n",
    "            for w1 in sentence:\n",
    "                model1[w1]+=1\n",
    "            # Bigrams\n",
    "            model2[None][None]=N\n",
    "            for w1, w2 in bigrams(sentence, pad_right=True, pad_left=True):\n",
    "                model2[w1][w2] += 1\n",
    "            # Trigrams\n",
    "            for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "                model3[(w1, w2)][w3] += 1\n",
    "\n",
    "        ## Transformar conteo a probabilidades\n",
    "        # Trigrams\n",
    "        for w1_w2 in model3:\n",
    "            w1, w2 = w1_w2[0], w1_w2[1]\n",
    "            total_count = model2[w1][w2]\n",
    "            for w3 in model3[w1_w2]:\n",
    "                model3[w1_w2][w3] /= total_count\n",
    "        # Bigrams        \n",
    "        for w1 in model2:\n",
    "            total_count = model1[w1]\n",
    "            for w2 in model2[w1]:\n",
    "                model2[w1][w2] /= total_count\n",
    "        # Unigrams    \n",
    "        total_count = sum(model1.values())\n",
    "        for w1 in model1:\n",
    "            model1[w1] /= total_count\n",
    "\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.model3 = model3     \n",
    "        \n",
    "    def predict_proba_trigam(self, lamb, trigram):\n",
    "        \"\"\"\n",
    "        Calcula la probabilidad de un trigrama usando interpolación lineal.\n",
    "            - p(w3|w1,w2) = lamb_1*q(w3|w1,w2)+lamb_2*q(w3|w2)+lamb_3*q(w3)\n",
    "            - lamb>=0, lamb_1+lamb_2+lamb_3=1\n",
    "        Parameters:\n",
    "            lamb: {array}, shape =3\n",
    "            hiperparámetros del modelo, cada componente debe ser positivo y deben sumar 1.\n",
    "\n",
    "        Returns:\n",
    "            score: float\n",
    "            probabilidad del trigrama\n",
    "        \"\"\"\n",
    "        w1, w2, w3 = trigram\n",
    "        proba = lamb[0]*self.model3[(w1,w2)][w3]+lamb[1]*self.model2[w2][w3]+lamb[2]*self.model1[w3]\n",
    "        return proba\n",
    "        \n",
    "    def get_perplexity(self, lamb, corpus):\n",
    "\n",
    "        \"\"\"\n",
    "        Obtiene la perplexity del modelo en un conjunto de validación\n",
    "            -perplexity = exp(NLL/N), donde NLL es la negative-log-likelihood del modelo y \n",
    "             N el número de trigramas en el corpus.\n",
    "\n",
    "        Parameters:\n",
    "            - corpus: {list of str}, shape (corpus_size) \n",
    "              corpus de validación tokenizado.\n",
    "            - lamb: {array}, shape =3\n",
    "              hiperparámetros del modelo, cada componente debe ser positivo y deben sumar 1.\n",
    "        Returns:\n",
    "            score: float\n",
    "            perplexity\n",
    "        \"\"\"\n",
    "\n",
    "        N = 0 #contador de trigramas del conjunto de validación\n",
    "        NLL = 0 #negative-log-likelihood\n",
    "        for sentence in corpus:\n",
    "            for trigram in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "                trigram_proba = self.predict_proba_trigam(lamb, trigram)\n",
    "                NLL = NLL - np.log(trigram_proba) \n",
    "                N = N+1\n",
    "        perplexity = np.exp(NLL/N)\n",
    "\n",
    "        return perplexity\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', '.', 'S', '.', 'And', 'Japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'Asia', \"'\", 's', 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far', '-', 'reaching', 'economic', 'damage', ',', 'businessmen', 'and', 'officials', 'said', '.']\n",
      "54716\n"
     ]
    }
   ],
   "source": [
    "corpus = reuters.sents()\n",
    "print(corpus[0])\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "model = Trigrams()\n",
    "%time model.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.383050732528654"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lamb = [0.7, 0.2, 0.1]\n",
    "%time model.get_perplexity(lamb, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ASIAN', 'EXPORTERS', 'FEAR')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7043479387228446"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calcular probabilidad de un trigrama\n",
    "example = trigrams(corpus[0], pad_right=True, pad_left=True)\n",
    "example_trigrams = []\n",
    "for trigram in example:\n",
    "    example_trigrams.append(trigram)\n",
    "    \n",
    "print(example_trigrams[2])\n",
    "model.predict_proba_trigam(lamb, example_trigrams[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
